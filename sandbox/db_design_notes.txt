Table 1: raw_data table - Stores the raw JSON data for each post, comment, and reply.
-post_id (CHAR(7)) (PK?)
-praw_data (JSONB)

Q: Should the post_id (ex: "1drx7sh") be the primary key (and also foreign key to post_id in the posts table), or should a serial id be the PK for better querying?
Maybe use a serial "extract_id" to give the option to re-extract posts later on if updated (despite this being unlikely), which would make post_id non-viable as PK?
^ This question applies for the posts table as well.
Note to self: Maybe also add extraction date to the json blob data using Python's datetime.
Note to self: A serialized id/extract_id may make more sense, because then I get extra protection if there are any errors in my extraction code.
Q: Have columns for "subreddit" and "extraction_date" or no? These may be columns in the posts table anyway, so would it be redundant to have them here as well
when each row is 1 to 1 with each row in the posts table, so if that data becomes needed, it could be found by a multi-table query? Note: "extraction_date" could differ
from the post's creation date, which is in unix time within the json blob.
Q: Since each row is 1 to 1 with each row in the posts table, should they just be combined as 1 table? Or does the large json_blob raw data column lower performance

*** Note: Change table to CHAR(7) from VARCHAR(10) ***

-----------------------------------------------------------------------------

Table 2: posts table - Stores flattened post data extracted from raw_data.
-post_id (CHAR(7)) (PK (and FK?))
-subreddit (VARCHAR(21))
-extraction_date (DATE)
-upvote_score (INT)
-title (VARCHAR(300))
-post_text (TEXT)

Table 2 potential extra columns added during transformation, such as with dbt:
-post_sentiment_score (FLOAT b/w 0-1 based off title and post_text)
-post_mh_score (FLOAT b/w 0-1 based off title and post_text)
-total_post_sentiment_score (FLOAT b/w 0-1 aggregating scores from post and all comments+replies)
-total_post_mh_score (FLOAT b/w 0-1 aggregating scores from post and all comments+replies)
...etc. scoring columns; may have both NLP/ML columns AND LLM-analysis score columns; TBD
-text_weight; maybe won't use this depending on how the transformation is set up
-top_keywords or top_phrases or similar

Q: Should the "potential extra columns" be an entirely new table, maybe something like "post_text_analysis," for scalability? If so, then which table would "subreddit" and
"extraction_date" columns go into; they would be part of the json blob flattening, which makes me think they should go in the posts table, but they are actually needed
for aggregating the scores from the post_text_analysis table into the final results/subreddits table; so in order to aggregate them if those 2 columns are in the posts
table, a multi-table query may be needed anyway; would this defeat the purpose of splitting into 2 tables.
Q: For title, should I just use TEXT instead of VARCHAR(300)? Is one better than the other for any reason? Max characters for a title is 300 characters.
Q: Should all 0-1/0-100% scores be floats (4 bytes, b/w 0 and 1), or smallints (2 bytes, b/w 0 and 100)?
Q: How to deal with "keywords" or "top_phrases" as the result would likely be an iterable, but each db value should be atomic?

-----------------------------------------------------------------------------

Table 3: responses table - Stores flattened comment and reply data extracted from raw_data.
response_id (PK)
post_id (CHAR(7)) (FK)
root_id (CHAR(7))
parent_id (VARCHAR(#) or NULL)
depth (SMALLINT)

Q: Should response_id and parent_id be CHAR or VARCHAR? They are 7 characters now, but will eventually expand to 8, but that may be in year. Ex: "l6wl0y9"
Q: Should post_id only be in Table 3a (instead of both Table 3a and 3b) to avoid data redundancy?

-----------------------------------------------------------------------------

Table 4: subreddits table - Stores results data