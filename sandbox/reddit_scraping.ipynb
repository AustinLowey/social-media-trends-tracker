{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract a PRAW post and visualize it using indents. ###\n",
    "\n",
    "import os\n",
    "import praw\n",
    "\n",
    "# Define the subreddit\n",
    "subreddit_name = 'news'\n",
    "\n",
    "# Setup PRAW credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"PRAW_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"PRAW_CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"PRAW_USER_AGENT\"),\n",
    ")\n",
    "\n",
    "def visualize_post(comments, depth=1, max_depth=3):\n",
    "    if depth > max_depth:\n",
    "        return\n",
    "    indent = '    ' * depth  # Increase indentation for each depth level\n",
    "    for comment in comments:\n",
    "        if isinstance(comment, praw.models.MoreComments):\n",
    "            continue\n",
    "        print(f\"{indent}{comment.score} upvotes: {comment.body[:100].replace(\"\\n\", \"\")}\")  # Truncate long comments\n",
    "        visualize_post(comment.replies, depth + 1, max_depth)\n",
    "\n",
    "# Get the top post from a subreddit\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "top_post = next(subreddit.top(time_filter='day', limit=1))\n",
    "\n",
    "top_post.comments.replace_more(limit=0)  # Load all comments\n",
    "print(f\"{top_post.score} upvotes: {top_post.title}\\n{top_post.selftext}\")\n",
    "visualize_post(top_post.comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract top posts from a specific subreddit and save as BSON files, while limiting tree depth and width. ###\n",
    "\n",
    "import os\n",
    "import praw\n",
    "import bson\n",
    "\n",
    "# Configure PRAW credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"PRAW_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"PRAW_CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"PRAW_USER_AGENT\"),\n",
    ")\n",
    "\n",
    "# Define the subreddit\n",
    "subreddit_name = 'all'\n",
    "\n",
    "# Create dir to save bson files\n",
    "directory = f\"./{subreddit_name}\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Define limit for the number of posts\n",
    "limit_posts = 10\n",
    "\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "def fetch_comments(submission, max_depth=3, max_top_level_comments=10, max_replies_per_comment=3):\n",
    "    \"\"\"Fetch comments from a submission with separate controls for depth, top-level comments, and replies.\"\"\"\n",
    "    submission.comments.replace_more(limit=0)  # Limit expansion of MoreComments\n",
    "\n",
    "    def fetch(comment_list, depth):\n",
    "        if depth > max_depth:\n",
    "            return []  # Stop recursion beyond max depth\n",
    "        local_comments = []\n",
    "        comment_limit = max_top_level_comments if depth == 1 else max_replies_per_comment\n",
    "        for comment in comment_list[:comment_limit]:  # Limit the number of comments processed at each depth\n",
    "            if isinstance(comment, praw.models.MoreComments):\n",
    "                continue  # Skip 'MoreComments' if any left\n",
    "            comment_data = {\n",
    "                'id': comment.id,\n",
    "                'author': str(comment.author),\n",
    "                'body': comment.body,\n",
    "                'created_utc': comment.created_utc,\n",
    "                'score': comment.score,\n",
    "                'replies': fetch(comment.replies, depth + 1)  # Recursive call to process replies\n",
    "            }\n",
    "            local_comments.append(comment_data)\n",
    "        return local_comments\n",
    "\n",
    "    # Start fetching comments from the top level\n",
    "    comments = fetch(submission.comments, 1)  # Start with depth 1\n",
    "    return comments\n",
    "\n",
    "for index, submission in enumerate(subreddit.top('day', limit=limit_posts)):\n",
    "    post_data = {\n",
    "        \"title\": submission.title,\n",
    "        \"text\": submission.selftext,\n",
    "        \"author\": str(submission.author),\n",
    "        \"score\": submission.score,\n",
    "        \"created_utc\": submission.created_utc,\n",
    "        \"num_comments\": submission.num_comments,\n",
    "        \"id\": submission.id,\n",
    "        \"url\": submission.url,\n",
    "        \"comments\": fetch_comments(submission)  # Fetch and store comments\n",
    "    }\n",
    "\n",
    "    # Serialize data to BSON\n",
    "    bson_data = bson.BSON.encode(post_data)\n",
    "    \n",
    "    # Save BSON data to a file\n",
    "    file_path = os.path.join(directory, f'post{index + 1}.bson')\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(bson_data)\n",
    "\n",
    "print(\"Data saved successfully to BSON files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today = datetime.date.today().isoformat()\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'remaining': None, 'reset_timestamp': None, 'used': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import praw\n",
    "\n",
    "# Configure PRAW credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"PRAW_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"PRAW_CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"PRAW_USER_AGENT\"),\n",
    ")\n",
    "\n",
    "reddit.auth.limits # Check current PRAW auth limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BSON to JSON ###\n",
    "\n",
    "import bson\n",
    "import json\n",
    "\n",
    "# file path\n",
    "bson_file_path = 'all/post1.bson'\n",
    "json_file_path = 'all/post1.json'\n",
    "\n",
    "# Read the BSON file\n",
    "with open(bson_file_path, 'rb') as file:\n",
    "    bson_data = file.read()\n",
    "    data_dict = bson.BSON.decode(bson_data)  # Decode BSON data to dict\n",
    "\n",
    "# Write data to JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(data_dict, json_file, indent=4)  # Serialize dictionary to JSON and save it\n",
    "\n",
    "print(f\"Converted {bson_file_path} to JSON and saved as {json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BSON to dict ###\n",
    "\n",
    "import bson\n",
    "\n",
    "def bson_to_dict(bson_file_path):\n",
    "    \"\"\"Read a BSON file and convert it to a Python dictionary.\"\"\"\n",
    "    with open(bson_file_path, 'rb') as file:\n",
    "        bson_data = file.read()\n",
    "        data_dict = bson.BSON.decode(bson_data)\n",
    "        return data_dict\n",
    "        \n",
    "bson_file_path = 'all/post1.bson'\n",
    "post_data = bson_to_dict(bson_file_path)\n",
    "print(post_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
